{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5472b7-3d27-41e2-9a43-9fce8a0dec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/rohansawant7978/grammar_correction/blob/main/05%20Vanilla%20Encoder%20Decoder(Word).ipynb.ipynb\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,RNN\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint,EarlyStopping,TensorBoard,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Nadam,Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk.translate.bleu_score as bleu\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import io\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6eb2336-e10c-4601-bc1f-3cb667b5f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_enc = pickle.load(open(r\"tokenizer_enc.pkl\", \"rb\"))\n",
    "tokenizer_dec = pickle.load(open(r\"tokenizer_dec.pkl\", \"rb\"))\n",
    "\n",
    "train_enc_inp = pickle.load(open(r\"train_enc_inp.pkl\", \"rb\"))\n",
    "val_enc_inp = pickle.load(open(r\"val_enc_inp.pkl\", \"rb\"))\n",
    "test_enc_inp = pickle.load(open(r\"test_enc_inp.pkl\", \"rb\"))\n",
    "\n",
    "train_dec_inp = pickle.load(open(r\"train_dec_inp.pkl\", \"rb\"))\n",
    "val_dec_inp = pickle.load(open(r\"val_dec_inp.pkl\", \"rb\"))\n",
    "test_dec_inp = pickle.load(open(r\"test_dec_inp.pkl\", \"rb\"))\n",
    "\n",
    "train_dec_out = pickle.load(open(r\"train_dec_out.pkl\", \"rb\"))\n",
    "val_dec_out = pickle.load(open(r\"val_dec_out.pkl\", \"rb\"))\n",
    "test_dec_out = pickle.load(open(r\"test_dec_out.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f03833a-5355-4431-9863-5af3bbd2eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,output_dim,enc_units,input_length):\n",
    "\n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_dim = output_dim\n",
    "        self.input_length = input_length\n",
    "        self.enc_units = enc_units\n",
    "        self.enc_output = 0\n",
    "        self.enc_state_h = 0\n",
    "        self.enc_state_c = 0\n",
    "\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.output_dim, \n",
    "                                   input_length=self.input_length,mask_zero=True,name=\"embedding_layer_encoder\")\n",
    "        \n",
    "        self.enc = LSTM(self.enc_units, return_state=True,return_sequences=True, name=\"Encoder_LSTM\")\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        input_embedding = self.embedding(input_sequence)\n",
    "        self.enc_output,self.enc_state_h,self.enc_state_c = self.enc(input_embedding,initial_state=states)\n",
    "\n",
    "        return self.enc_output, self.enc_state_h,self.enc_state_c\n",
    "\n",
    "    def initialize_states(self,batch_size):\n",
    "        ini_hidden_state = tf.zeros(shape=(batch_size,self.enc_units))\n",
    "        ini_cell_state = tf.zeros(shape=(batch_size,self.enc_units))\n",
    "\n",
    "        return ini_hidden_state,ini_cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd0400be-46bc-4d85-a371-12c7b0d86065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,output_dim,dec_units,input_length):\n",
    "\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_dim = output_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.input_length = input_length\n",
    "\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.output_dim,\n",
    "                                   input_length=self.input_length,mask_zero=True,name=\"embedding_layer_decoder\")\n",
    "        \n",
    "        self.dec = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Decoder_LSTM\")\n",
    "        \n",
    "    def call(self,input_sequence,initial_states):\n",
    "        target_embedding = self.embedding(input_sequence)\n",
    "        dec_output,dec_state_h,dec_state_c = self.dec(target_embedding, initial_state=initial_states)\n",
    "        return dec_output,dec_state_h,dec_state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c2a047-f906-4787-8a63-f7b83e7e4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self,enc_vocab_size,enc_output_dim,enc_inp_length,enc_units,\n",
    "                 dec_vocab_size,dec_output_dim,dec_inp_length,dec_units,batch_size):\n",
    "        \n",
    "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
    "        self.encoder = Encoder(vocab_size=enc_vocab_size, output_dim=enc_output_dim, \n",
    "                               input_length=enc_inp_length, enc_units=enc_units)\n",
    "        self.decoder = Decoder(vocab_size=dec_vocab_size, output_dim=dec_output_dim, \n",
    "                               input_length=dec_inp_length, dec_units=dec_units)\n",
    "        self.dense   = Dense(dec_vocab_size, activation='softmax')\n",
    "        self.ini_states = self.encoder.initialize_states(batch_size=batch_size)\n",
    "\n",
    "    def call(self,data):\n",
    "        input,output = data[0], data[1]\n",
    "        enc_output,enc_h,enc_c = self.encoder(input,self.ini_states)\n",
    "        dec_output,dec_h,dec_c = self.decoder(output, [enc_h, enc_c])\n",
    "        output = self.dense(dec_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef44da97-4d40-403b-935c-501de081059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56656b3d-f5a6-403c-a0e9-d92370e76392",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_voc_size = len(tokenizer_enc.word_index) + 1 \n",
    "dec_voc_size = len(tokenizer_dec.word_index) + 1\n",
    "embedd_dim = 100\n",
    "enc_inp_len = 12\n",
    "dec_inp_len = 13\n",
    "lstm_size=64\n",
    "batch_size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffff126c-29f2-489e-8ef2-610274d37472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_trunc_idx = (train_enc_inp.shape[0]//batch_size)*batch_size \n",
    "val_trunc_idx = (val_enc_inp.shape[0]//batch_size)*batch_size \n",
    "\n",
    "train_enc_inp_truncated = train_enc_inp[:train_trunc_idx]\n",
    "train_dec_inp_truncated = train_dec_inp[:train_trunc_idx]\n",
    "train_dec_out_truncated = train_dec_out[:train_trunc_idx]\n",
    "\n",
    "val_enc_inp_truncated = val_enc_inp[:val_trunc_idx]\n",
    "val_dec_inp_truncated = val_dec_inp[:val_trunc_idx]\n",
    "val_dec_out_truncated = val_dec_out[:val_trunc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b96e23-1ebb-46a4-b867-42d941ff1fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 07:27:58.312201: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-11-26 07:27:58.312242: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-11-26 07:27:58.312250: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-11-26 07:27:58.312297: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-26 07:27:58.312323: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model  = Encoder_decoder(enc_vocab_size=enc_voc_size,enc_output_dim=embedd_dim,\n",
    "                         enc_inp_length=enc_inp_len,enc_units=lstm_size,\n",
    "                         dec_vocab_size=dec_voc_size,dec_output_dim=embedd_dim,\n",
    "                         dec_inp_length=dec_inp_len,dec_units=lstm_size,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "#Tensorboard\n",
    "!rm -rf ./logs/fit\n",
    "IST = pytz.timezone('Etc/GMT+3')\n",
    "log_dir=f'05_InitialEncodeDecoder/tb_logs/{datetime.now(IST).strftime(\"%Y%m%d%H%M%S\")}'\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=1,mode='min')\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', min_delta=0.001, patience=5, verbose=1, factor=0.9)\n",
    "check_point = ModelCheckpoint('05_InitialEncodeDecoder', monitor='val_loss',\n",
    "                              save_best_only=True, save_weights_only=True,mode='min', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e3ce69-437c-4b94-9bb2-ab0a6a03ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),loss=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "392aba9d-6c52-4a25-bde0-50d4c31ff2f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorboard_callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39m[train_enc_inp_truncated,train_dec_inp_truncated],y\u001b[38;5;241m=\u001b[39mtrain_dec_out_truncated,\n\u001b[1;32m      2\u001b[0m           validation_data\u001b[38;5;241m=\u001b[39m([val_enc_inp_truncated,val_dec_inp_truncated],val_dec_out_truncated),\n\u001b[0;32m----> 3\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,callbacks\u001b[38;5;241m=\u001b[39m[\u001b[43mtensorboard_callback\u001b[49m,earlystop,reducelr,check_point])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensorboard_callback' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(x=[train_enc_inp_truncated,train_dec_inp_truncated],y=train_dec_out_truncated,\n",
    "          validation_data=([val_enc_inp_truncated,val_dec_inp_truncated],val_dec_out_truncated),\n",
    "          epochs=30, batch_size=batch_size,callbacks=[tensorboard_callback,earlystop,reducelr,check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bed6c8-529f-45c9-acf3-f7dcf6b108d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ba978-4a0a-4d7f-9ce1-0bc7f32ce6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88871064-3987-4212-9009-bc7e2a0ac81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
